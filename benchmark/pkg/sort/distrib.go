package sort

import (
	"fmt"
	"io"
	"math"
	"runtime"
	"sync"

	"github.com/nathantp/gpu-radix-sort/benchmark/pkg/data"
	"github.com/pkg/errors"
)

// Read InBkts in order and sort by the radix of width width and starting at
// offset Returns a distributed array (generated by 'factory') with one part
// per unique radix value. Array names will be prefixed with baseName.
type DistribWorker func(inBkts []*data.PartRef, offset int, width int, baseName string, factory *data.ArrayFactory) (data.DistribArray, error)

// XXX Since DistribArray's were refactored to have fixed-sized partitions, we
// can no longer pre-create it on the host, the client has to do it since only
// the client knows the partition sizes. This is fine with local workers, but
// Docker permissions make it difficult in FaaS. We're going to have to figure
// something out to make files created on the worker readible by the host
// (maybe just 777 for now, not ideal).

// Returns a DistribWorker that uses mgr to sort via FaaS
// func InitFaasWorker(mgr *srkmgr.SrkManager) DistribWorker {
// 	return func(inBkts []*data.PartRef,
// 		offset int, width int, baseName string,
// 		factory data.ArrayFactory) (data.DistribArray, error) {
//
// 		var err error
//
// 		nBucket := 1 << width
//
// 		faasRefs := make([]*faas.FaasFilePartRef, len(inBkts))
// 		for i, bktRef := range inBkts {
// 			faasRefs[i], err = faas.FilePartRefToFaas(bktRef)
// 		}
//
// 		// Generate output array on host side to avoid permissions errors from Docker
// 		shape := data.CreateShape
// 		outArr, err := factory.Create(baseName+"_output", nBucket)
// 		if err != nil {
// 			return nil, errors.Wrap(err, "Could not allocate output")
// 		}
//
// 		fileArr, ok := outArr.(*data.FileDistribArray)
// 		if !ok {
// 			return nil, fmt.Errorf("Unsupported DistribArray type %T: Only FileRefPart's are supported", outArr)
// 		}
//
// 		faasArg := &faas.FaasArg{
// 			Offset:  offset,
// 			Width:   width,
// 			ArrType: "file",
// 			Input:   faasRefs,
// 			Output:  filepath.Base(fileArr.RootPath),
// 		}
//
// 		err = faas.InvokeFaasSort(mgr, faasArg)
// 		if err != nil {
// 			return nil, errors.Wrap(err, "FaaS sort failure")
// 		}
//
// 		return outArr, nil
// 	}
// }

func LocalDistribWorker(inBkts []*data.PartRef, offset int, width int, baseName string, factory *data.ArrayFactory) (data.DistribArray, error) {
	var err error

	totalLen := 0
	for i := 0; i < len(inBkts); i++ {
		totalLen += inBkts[i].NByte
	}

	inBytes, err := data.FetchPartRefs(inBkts)
	if err != nil {
		return nil, errors.Wrap(err, "Couldn't read input references")
	}

	// Actual Sort
	nBucket := 1 << width
	boundaries := make([]uint64, nBucket)
	if err := GpuPartial(inBytes, boundaries, offset, width); err != nil {
		return nil, errors.Wrap(err, "Local sort failed")
	}

	partSzs := make([]int, nBucket)
	for i := 0; i < nBucket; i++ {
		if i == nBucket-1 {
			partSzs[i] = len(inBytes) - (int)(boundaries[i])
		} else {
			partSzs[i] = (int)(boundaries[i+1]) - (int)(boundaries[i])
		}
	}

	shape := data.CreateShape(partSzs)

	// Write Outputs
	outArr, err := factory.Create(baseName+"_output", shape)
	if err != nil {
		return nil, errors.Wrap(err, "Could not allocate output")
	}

	for i := 0; i < nBucket; i++ {
		start := (int)(boundaries[i])
		end := start + partSzs[i]

		writer, err := outArr.GetPartWriter(i)
		if err != nil {
			return nil, errors.Wrapf(err, "Failed to write bucket %v", i)
		}

		n, err := writer.Write(inBytes[start:end])
		if err != nil && err != io.EOF {
			writer.Close()
			return nil, errors.Wrap(err, "Could not write to output")
		}
		if n != end-start {
			writer.Close()
			return nil, fmt.Errorf("Could not write enough bytes to output: wanted %v, got %v", partSzs[i], n)
		}
		writer.Close()
	}

	return outArr, nil
}

// Distributed sort of arr. The bytes in arr will be interpreted as uint32's
// Returns an ordered list of distributed arrays containing the sorted output
// (concatenate each array's partitions in order to get final result). 'len' is
// the number of bytes in arr.
func SortDistribFromArr(arr data.DistribArray, sz int, baseName string,
	factory *data.ArrayFactory, worker DistribWorker) ([]data.DistribArray, error) {
	// Data Layout:
	//	 - Distrib Arrays store all output from a single node
	//	 - DistribParts represent radix sort buckets (there will be nbucket parts per DistribArray)
	//
	// Basic algorithm/schema:
	//   - Inputs: each worker recieves as input a reference to the
	//     DistribParts it should consume. The first partition may include an
	//     offset to start reading from. Likewise, the last partition may include
	//     an offest to stop reading at. Intermediate partitions are read in
	//     their entirety.
	//	 - Outputs: Each worker will output a DistribArray with one partition
	//	   per radix bucket. Partitions may have zero length, but they will
	//	   always exist.
	//	 - Input distribArrays may be garbage collected after every worker has
	//     provided their output (output distribArrays are copies, not references).
	nworker := 2 //number of workers (degree of parallelism)
	// nworker := 1 //number of workers (degree of parallelism)
	// width := 4 //number of bits to sort per round
	width := 8 //number of bits to sort per round
	// width := 16           //number of bits to sort per round
	nstep := (32 / width) // number of steps needed to fully sort

	// Target number of bytes to process per worker, the last worker might get less
	nElem := sz / 4
	maxPerWorker := (int)(math.Ceil((float64)(nElem)/(float64)(nworker))) * 4

	// Initial input is the output for "step -1"
	var outputs []data.DistribArray
	outputs = []data.DistribArray{arr}

	for step := 0; step < nstep; step++ {
		inputs := outputs
		outputs = make([]data.DistribArray, nworker)

		// This is perhaps over-optimization but it shaves ~6GB off the
		// resident memory size for MemDistribArrays in the big test (13 vs
		// 19). It likely would have little effect on other sorts of
		// DistribArrays and has little/no performance benefit.
		// XXX after the refactor, how important is this? Should I just put it in MemDistribArray.Destroy()?
		runtime.GC()

		inGen, err := NewBucketReader(inputs, STRIDED)
		if err != nil {
			return nil, err
		}

		var wg sync.WaitGroup
		wg.Add(nworker)
		errChan := make(chan error, nworker)
		for workerId := 0; workerId < nworker; workerId++ {
			// Repartition previous output
			workerInputs, genErr := inGen.ReadRef(maxPerWorker)
			if genErr == io.EOF && workerId+1 != nworker {
				return nil, errors.New("Premature EOF from input generator")
			} else if genErr != nil && genErr != io.EOF {
				return nil, errors.Wrap(err, "Input generator had an error")
			}

			go func(id int, inputs []*data.PartRef) {
				defer wg.Done()

				workerName := fmt.Sprintf("%v_step%v_worker%v", baseName, step, id)

				outputs[id], err = worker(inputs, step*width, width, workerName, factory)

				if err != nil {
					errChan <- errors.Wrapf(err, "Worker failure on step %v, worker %v", step, id)
					return
				}
			}(workerId, workerInputs)
		}
		wg.Wait()
		select {
		case firstErr := <-errChan:
			return nil, errors.Wrapf(firstErr, "Worker failure")
		default:
		}
	}

	return outputs, nil
}

// Sort a native byte array using DistribArrays from factory and remote worker
// invoker 'worker'.
func SortDistribFromRaw(inRaw []byte, baseName string,
	factory *data.ArrayFactory, worker DistribWorker) ([]byte, error) {
	var err error

	err = InitLibSort()
	if err != nil {
		return nil, errors.Wrap(err, "Failed to initialize libsort")
	}

	shape := data.CreateShapeUniform(len(inRaw), 1)
	origArr, err := factory.Create(baseName+"_input", shape)
	if err != nil {
		return nil, errors.Wrap(err, "failed to create input distribarray")
	}

	writer, err := origArr.GetPartWriter(0)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to get writer for partition")
	}

	n, err := writer.Write(inRaw)
	if err != nil {
		return nil, errors.Wrap(err, "error writing initial data")
	} else if n != len(inRaw) {
		return nil, fmt.Errorf("Could not write entire initial data")
	}
	writer.Close()

	outArrs, err := SortDistribFromArr(origArr, len(inRaw), baseName, factory, worker)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to sort distribArrays")
	}

	reader, err := NewBucketReader(outArrs, STRIDED)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to get reader for output")
	}

	outRaw := make([]byte, len(inRaw))
	// We don't use ioutil.ReadAll because we know the size of the output already
	for n := 0; n < len(inRaw); {
		nCur, err := reader.Read(outRaw[n:])
		if err != nil {
			return nil, errors.Wrap(err, "Failed to read results")
		}
		n += nCur
	}

	var destroyErr error
	for i := 0; i < len(outArrs); i++ {
		if err = outArrs[i].Destroy(); err != nil {
			destroyErr = err
		}
	}

	if destroyErr != nil {
		return outRaw, errors.Wrapf(err, "Failed to clean up one or more arrays")
	}

	return outRaw, nil
}
